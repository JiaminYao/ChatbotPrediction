{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a04504e-4021-45ae-8788-580d052d089c",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c3f52-e1fb-4978-81ce-933f9c928358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:54:45.947476: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-02 19:54:45.947549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-02 19:54:45.949372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-02 19:54:45.959606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import os, tensorflow as tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "from evaluation import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92942f37-3dbc-4623-aed3-d86bbd2debad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "    Dense, Dropout, Input, MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d36760-c4a4-4fe8-95d5-169bdd75e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv A Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis refers to a situation where a country faces severe shortage of foreign currencies, usually dollars or euros\n",
      "Conv B Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis, also known as a currency crisis or balance of payments crisis, occurs when a country's currency experience\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class CFG:\n",
    "        seeds = [42, 119, 2020, 2024, 2028]\n",
    "        \n",
    "train_df, test_df, y, class_names = load_and_prepare_data()\n",
    "pairs_train, pairs_val, test_pairs, y_train, y_val = prepare_dual_conversation_pipeline(train_df, test_df, y)\n",
    "\n",
    "print(\"Conv A Example:\\n\", pairs_train[0][0][:200])\n",
    "print(\"Conv B Example:\\n\", pairs_train[0][1][:200])\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2742a2d4-9703-4121-b587-90d0c16518f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:55:01.718849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38367 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer (shared across the two inputs)\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 20000\n",
    "max_length = 512\n",
    "\n",
    "adapt_strings = [p[0] for p in pairs_train] + [p[1] for p in pairs_train]\n",
    "adapt_ds = tf.data.Dataset.from_tensor_slices([str(t) for t in adapt_strings]).batch(1024)\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorizer.adapt(adapt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473d51b2-47dd-41c2-8710-ae061016ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data pipelines for ((A,B), y)\n",
    "def make_dual_dataset(pairs, labels=None, batch_size=128, training=True):\n",
    "    part_a = [str(p[0]) for p in pairs]\n",
    "    part_b = [str(p[1]) for p in pairs]\n",
    "\n",
    "    inputs = {\"inp_a\": tf.constant(part_a), \"inp_b\": tf.constant(part_b)}\n",
    "\n",
    "    if labels is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    else:\n",
    "        labels = np.asarray(labels, dtype=np.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "    if labels is not None and training:\n",
    "        ds = ds.shuffle(2048, reshuffle_each_iteration=True)\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dual_dataset(pairs_train, y_train, training=True)\n",
    "val_ds   = make_dual_dataset(pairs_val,   y_val,   training=False)\n",
    "test_ds  = make_dual_dataset(test_pairs,  labels=None,  training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62a1b71-9c02-4b24-8b25-c79cec2bdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (two string inputs â†’ vectorizer â†’ shared embedding â†’ concat â†’ conv)\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3):\n",
    "    inp_a = Input(shape=(), dtype=tf.string, name=\"inp_a\")\n",
    "    inp_b = Input(shape=(), dtype=tf.string, name=\"inp_b\")\n",
    "\n",
    "    # shared layers\n",
    "    emb   = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    # branch A\n",
    "    xa = text_vectorizer(inp_a)\n",
    "    xa = emb(xa)\n",
    "    xa = Conv1D(32, 3, activation=\"relu\")(xa)\n",
    "    xa = Conv1D(32, 3, activation=\"relu\")(xa)\n",
    "    xa = MaxPooling1D()(xa)\n",
    "    xa = Conv1D(64, 3, activation=\"relu\")(xa)\n",
    "    xa = GlobalMaxPooling1D()(xa)\n",
    "\n",
    "    # branch B\n",
    "    xb = text_vectorizer(inp_b)\n",
    "    xb = emb(xb)\n",
    "    xb = Conv1D(32, 3, activation=\"relu\")(xb)\n",
    "    xb = Conv1D(32, 3, activation=\"relu\")(xb)\n",
    "    xb = MaxPooling1D()(xb)\n",
    "    xb = Conv1D(64, 3, activation=\"relu\")(xb)\n",
    "    xb = GlobalMaxPooling1D()(xb)\n",
    "\n",
    "    # merge\n",
    "    x  = Concatenate()([xa, xb])\n",
    "    x  = Dropout(0.3)(x)\n",
    "    x  = Dense(128, activation=\"swish\")(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_a, inp_b], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da7f1d1-23be-41a0-9f7b-4c4c61bcc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training new model for seed 42...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:55:09.484267: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-02 19:55:09.962267: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-11-02 19:55:10.069566: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-02 19:55:11.053319: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ffd0ea121d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-02 19:55:11.053365: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2025-11-02 19:55:11.060779: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762134911.142513 1137214 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - ETA: 0s - loss: 1.0896 - accuracy: 0.3825\n",
      "Epoch 1: val_loss improved from inf to 1.07090, saving model to models_cnn_dual/cnn_dual_seed_42.keras\n",
      "360/360 [==============================] - 41s 103ms/step - loss: 1.0896 - accuracy: 0.3825 - val_loss: 1.0709 - val_accuracy: 0.4231\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0378 - accuracy: 0.4671\n",
      "Epoch 2: val_loss did not improve from 1.07090\n",
      "360/360 [==============================] - 25s 71ms/step - loss: 1.0378 - accuracy: 0.4671 - val_loss: 1.0729 - val_accuracy: 0.4422\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8930 - accuracy: 0.5804\n",
      "Epoch 3: val_loss did not improve from 1.07090\n",
      "360/360 [==============================] - 20s 57ms/step - loss: 0.8930 - accuracy: 0.5804 - val_loss: 1.2097 - val_accuracy: 0.4313\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.6946\n",
      "Epoch 4: val_loss did not improve from 1.07090\n",
      "360/360 [==============================] - 16s 46ms/step - loss: 0.6939 - accuracy: 0.6946 - val_loss: 1.5245 - val_accuracy: 0.4151\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.7840\n",
      "Epoch 5: val_loss did not improve from 1.07090\n",
      "360/360 [==============================] - 13s 36ms/step - loss: 0.5146 - accuracy: 0.7840 - val_loss: 2.0018 - val_accuracy: 0.4159\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.8434\n",
      "Epoch 6: val_loss did not improve from 1.07090\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 12s 33ms/step - loss: 0.3913 - accuracy: 0.8434 - val_loss: 2.3558 - val_accuracy: 0.4053\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 42): 128.63 seconds (2.14 minutes)\n",
      "[Seed 42] Val Loss: 1.0709 | Val Acc: 42.31%\n",
      "\n",
      "ðŸš€ Training new model for seed 119...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0916 - accuracy: 0.3822\n",
      "Epoch 1: val_loss improved from inf to 1.07619, saving model to models_cnn_dual/cnn_dual_seed_119.keras\n",
      "360/360 [==============================] - 32s 84ms/step - loss: 1.0916 - accuracy: 0.3822 - val_loss: 1.0762 - val_accuracy: 0.4285\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0314 - accuracy: 0.4743\n",
      "Epoch 2: val_loss did not improve from 1.07619\n",
      "360/360 [==============================] - 20s 57ms/step - loss: 1.0314 - accuracy: 0.4743 - val_loss: 1.0784 - val_accuracy: 0.4354\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.6011\n",
      "Epoch 3: val_loss did not improve from 1.07619\n",
      "360/360 [==============================] - 16s 45ms/step - loss: 0.8621 - accuracy: 0.6011 - val_loss: 1.2449 - val_accuracy: 0.4213\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6586 - accuracy: 0.7111\n",
      "Epoch 4: val_loss did not improve from 1.07619\n",
      "360/360 [==============================] - 14s 38ms/step - loss: 0.6586 - accuracy: 0.7111 - val_loss: 1.5598 - val_accuracy: 0.4040\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.7899\n",
      "Epoch 5: val_loss did not improve from 1.07619\n",
      "360/360 [==============================] - 11s 31ms/step - loss: 0.4998 - accuracy: 0.7899 - val_loss: 1.9044 - val_accuracy: 0.4038\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8430\n",
      "Epoch 6: val_loss did not improve from 1.07619\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 10s 29ms/step - loss: 0.3899 - accuracy: 0.8430 - val_loss: 2.4051 - val_accuracy: 0.4013\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 119): 104.41 seconds (1.74 minutes)\n",
      "[Seed 119] Val Loss: 1.0762 | Val Acc: 42.85%\n",
      "\n",
      "ðŸš€ Training new model for seed 2020...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0908 - accuracy: 0.3797\n",
      "Epoch 1: val_loss improved from inf to 1.08103, saving model to models_cnn_dual/cnn_dual_seed_2020.keras\n",
      "360/360 [==============================] - 31s 81ms/step - loss: 1.0908 - accuracy: 0.3797 - val_loss: 1.0810 - val_accuracy: 0.4133\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0452 - accuracy: 0.4592\n",
      "Epoch 2: val_loss did not improve from 1.08103\n",
      "360/360 [==============================] - 20s 56ms/step - loss: 1.0452 - accuracy: 0.4592 - val_loss: 1.0907 - val_accuracy: 0.4395\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.5701\n",
      "Epoch 3: val_loss did not improve from 1.08103\n",
      "360/360 [==============================] - 17s 47ms/step - loss: 0.9121 - accuracy: 0.5701 - val_loss: 1.1848 - val_accuracy: 0.4278\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.6899\n",
      "Epoch 4: val_loss did not improve from 1.08103\n",
      "360/360 [==============================] - 13s 35ms/step - loss: 0.7073 - accuracy: 0.6899 - val_loss: 1.4585 - val_accuracy: 0.4167\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7791\n",
      "Epoch 5: val_loss did not improve from 1.08103\n",
      "360/360 [==============================] - 11s 31ms/step - loss: 0.5288 - accuracy: 0.7791 - val_loss: 1.8269 - val_accuracy: 0.4007\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8393\n",
      "Epoch 6: val_loss did not improve from 1.08103\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 9s 26ms/step - loss: 0.4041 - accuracy: 0.8393 - val_loss: 2.1712 - val_accuracy: 0.3996\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 2020): 101.20 seconds (1.69 minutes)\n",
      "[Seed 2020] Val Loss: 1.0810 | Val Acc: 41.33%\n",
      "\n",
      "ðŸš€ Training new model for seed 2024...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0907 - accuracy: 0.3798\n",
      "Epoch 1: val_loss improved from inf to 1.06938, saving model to models_cnn_dual/cnn_dual_seed_2024.keras\n",
      "360/360 [==============================] - 31s 81ms/step - loss: 1.0907 - accuracy: 0.3798 - val_loss: 1.0694 - val_accuracy: 0.4322\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0409 - accuracy: 0.4644\n",
      "Epoch 2: val_loss did not improve from 1.06938\n",
      "360/360 [==============================] - 21s 58ms/step - loss: 1.0409 - accuracy: 0.4644 - val_loss: 1.0865 - val_accuracy: 0.4336\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8973 - accuracy: 0.5805\n",
      "Epoch 3: val_loss did not improve from 1.06938\n",
      "360/360 [==============================] - 15s 42ms/step - loss: 0.8973 - accuracy: 0.5805 - val_loss: 1.1966 - val_accuracy: 0.4320\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.7033\n",
      "Epoch 4: val_loss did not improve from 1.06938\n",
      "360/360 [==============================] - 13s 37ms/step - loss: 0.6852 - accuracy: 0.7033 - val_loss: 1.5271 - val_accuracy: 0.4252\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.7955\n",
      "Epoch 5: val_loss did not improve from 1.06938\n",
      "360/360 [==============================] - 11s 31ms/step - loss: 0.5011 - accuracy: 0.7955 - val_loss: 1.9184 - val_accuracy: 0.4097\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8510\n",
      "Epoch 6: val_loss did not improve from 1.06938\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 9s 26ms/step - loss: 0.3798 - accuracy: 0.8510 - val_loss: 2.2311 - val_accuracy: 0.4163\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 2024): 100.37 seconds (1.67 minutes)\n",
      "[Seed 2024] Val Loss: 1.0694 | Val Acc: 43.22%\n",
      "\n",
      "ðŸš€ Training new model for seed 2028...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0888 - accuracy: 0.3873\n",
      "Epoch 1: val_loss improved from inf to 1.07566, saving model to models_cnn_dual/cnn_dual_seed_2028.keras\n",
      "360/360 [==============================] - 30s 78ms/step - loss: 1.0888 - accuracy: 0.3873 - val_loss: 1.0757 - val_accuracy: 0.4171\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0283 - accuracy: 0.4737\n",
      "Epoch 2: val_loss did not improve from 1.07566\n",
      "360/360 [==============================] - 22s 61ms/step - loss: 1.0283 - accuracy: 0.4737 - val_loss: 1.0907 - val_accuracy: 0.4362\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.5938\n",
      "Epoch 3: val_loss did not improve from 1.07566\n",
      "360/360 [==============================] - 17s 47ms/step - loss: 0.8715 - accuracy: 0.5938 - val_loss: 1.2251 - val_accuracy: 0.4288\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.7062\n",
      "Epoch 4: val_loss did not improve from 1.07566\n",
      "360/360 [==============================] - 13s 36ms/step - loss: 0.6753 - accuracy: 0.7062 - val_loss: 1.5195 - val_accuracy: 0.4161\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7933\n",
      "Epoch 5: val_loss did not improve from 1.07566\n",
      "360/360 [==============================] - 10s 29ms/step - loss: 0.5001 - accuracy: 0.7933 - val_loss: 1.8912 - val_accuracy: 0.4207\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8475\n",
      "Epoch 6: val_loss did not improve from 1.07566\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 10s 27ms/step - loss: 0.3841 - accuracy: 0.8475 - val_loss: 2.2739 - val_accuracy: 0.4031\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 2028): 101.65 seconds (1.69 minutes)\n",
      "[Seed 2028] Val Loss: 1.0757 | Val Acc: 41.71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train one model or an ensemble over seeds\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "os.makedirs(\"models_cnn_dual0\", exist_ok=True)\n",
    "cnn_models = []\n",
    "\n",
    "for seed in CFG.seeds:\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    model_path = os.path.join(\"models_cnn_dual0\", f\"cnn_dual_seed_{seed}.keras\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"âœ… Found existing model for seed {seed}, loading...\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        print(f\"ðŸš€ Training new model for seed {seed}...\")\n",
    "        model = get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Measure training time ---\n",
    "        t0 = time.time()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=30,\n",
    "            callbacks=[ckpt, es],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        t1 = time.time()\n",
    "        print(f\"â±ï¸ CNN Training time (seed {seed}): {(t1 - t0):.2f} seconds ({(t1 - t0)/60:.2f} minutes)\")\n",
    "        \n",
    "        # --- Reload best model (for consistency) ---\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # --- Evaluate on validation set ---\n",
    "    loss, acc = model.evaluate(val_ds, verbose=0)\n",
    "    print(f\"[Seed {seed}] Val Loss: {loss:.4f} | Val Acc: {acc*100:.2f}%\\n\")\n",
    "\n",
    "    # --- Store model for ensemble ---\n",
    "    cnn_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56514fb-870a-4c61-b9b3-c764b67ebd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in cnn_models: 5\n",
      "Model 0 predictions shape: (11496, 3)\n",
      "Model 1 predictions shape: (11496, 3)\n",
      "Model 2 predictions shape: (11496, 3)\n",
      "Model 3 predictions shape: (11496, 3)\n",
      "Model 4 predictions shape: (11496, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of models in cnn_models: {len(cnn_models)}\")\n",
    "if len(cnn_models) > 0:\n",
    "    for i, m in enumerate(cnn_models):\n",
    "        preds = m.predict(val_ds, verbose=0)\n",
    "        print(f\"Model {i} predictions shape:\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc2bc23-f8e5-4abb-b723-d8464a6806cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (CNN)\n",
    "y_proba_val_cnn = np.mean([m.predict(val_ds,  verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_val_cnn  = y_proba_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Test predictions + submission\n",
    "test_ds = make_dual_dataset(test_pairs, labels=None, training=False)\n",
    "\n",
    "y_proba_test_cnn = np.mean([m.predict(test_ds, verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_test_cnn  = y_proba_test_cnn.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9f0b37-b10a-4f5b-bfa2-6906fcdd2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ CNN EVALUATION ================\n",
      "\n",
      "*** GLOBAL METRICS ***\n",
      "Accuracy (Global)      : 0.4467\n",
      "Precision (Macro Avg)  : 0.4522\n",
      "Recall (Macro Avg)     : 0.4381\n",
      "F1-Score (Macro Avg)   : 0.4201\n",
      "\n",
      "*** PER-CLASS EVALUATION ***\n",
      "Class                Precision    Recall  F1-Score   Support\n",
      "------------------------------------------------------------\n",
      "winner_model_a            0.46      0.50      0.48      4013\n",
      "winner_model_b            0.43      0.62      0.51      3931\n",
      "winner_tie                0.46      0.19      0.27      3552\n",
      "------------------------------------------------------------\n",
      "Macro Avg                 0.45      0.44      0.42     34488\n",
      "Weighted Avg              0.45      0.45      0.43     34488\n",
      "\n",
      "*** ROC-AUC EVALUATION ***\n",
      "ROC-AUC (OvR) : 0.6165\n",
      "\n",
      "*** LOG-LOSS EVALUATION ***\n",
      "Log-loss      : 1.0684\n",
      "\n",
      "*** LOG-LOSS PER CLASS ***\n",
      "Class 0: 1.0398  (n=4013)\n",
      "Class 1: 1.0068  (n=3931)\n",
      "Class 2: 1.1689  (n=3552)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ CNN0 EVALUATION ================\\n\")\n",
    "# Metrics\n",
    "_ = eval_metrics(y_val, y_pred_val_cnn)\n",
    "eval_classification_report(y_val, y_pred_val_cnn, class_names)\n",
    "# ROC-AUC\n",
    "_ = eval_roc_auc(y_val, y_proba_val_cnn)\n",
    "# Log-loss\n",
    "_ = eval_log_loss(y_val, y_proba_val_cnn)\n",
    "_ = eval_log_loss_per_class(y_val, y_proba_val_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880a1e6c-1a5b-436f-bed9-2f29d04433f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      " [[2023 1602  388]\n",
      " [1104 2434  393]\n",
      " [1256 1618  678]]\n",
      "Saved plot to: images/confusion_matrix/confusion_matrix_cnn.png\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix + Plot\n",
    "cm_cnn = eval_confusion_matrix(y_val, y_pred_val_cnn, n_classes=y_proba_val_cnn.shape[1])\n",
    "plot_confusion_matrix(cm_cnn, class_names, title=\"Confusion Matrix â€” CNN0\", save_path=\"results/confusion_matrix/confusion_matrix_cnn0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4f7042-724d-48b0-9a67-9797aac5ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: images/roc/roc_cnn.png\n"
     ]
    }
   ],
   "source": [
    "# ROC Curves\n",
    "plot_roc_curves(y_val, y_proba_val_cnn, class_names, title_prefix=\"CNN0 ROC\", save_path=\"results/roc/roc_cnn0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91e46aa-3068-40d2-89e9-46b1406caade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC data for class 0 (AUC=0.6369) â†’ results/roc/CNN_fold1_class0.csv\n",
      "Saved ROC data for class 1 (AUC=0.6364) â†’ results/roc/CNN_fold1_class1.csv\n",
      "Saved ROC data for class 2 (AUC=0.5761) â†’ results/roc/CNN_fold1_class2.csv\n"
     ]
    }
   ],
   "source": [
    "save_roc_to_csv(y_val, y_proba_val_cnn, \"CNN0\", fold_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb6dc0b-095c-47af-9c1e-4328cc34bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: results/submission/submission_cnn.csv\n"
     ]
    }
   ],
   "source": [
    "submission_cnn = build_submission(\n",
    "    test_df=test_df,\n",
    "    y_pred_test=y_pred_test_cnn,\n",
    "    y_proba_test=y_proba_test_cnn,\n",
    "    model_name=\"cnn0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649f1bb-64aa-471b-87f8-253c4d98c253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
