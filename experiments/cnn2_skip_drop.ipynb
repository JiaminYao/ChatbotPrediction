{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a04504e-4021-45ae-8788-580d052d089c",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c3f52-e1fb-4978-81ce-933f9c928358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 01:51:25.455331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-16 01:51:25.455406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-16 01:51:25.456501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-16 01:51:25.464059: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import os, tensorflow as tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "from evaluation import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92942f37-3dbc-4623-aed3-d86bbd2debad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "    Dense, Dropout, Input, MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d36760-c4a4-4fe8-95d5-169bdd75e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv A Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis refers to a situation where a country faces severe shortage of foreign currencies, usually dollars or euros\n",
      "Conv B Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis, also known as a currency crisis or balance of payments crisis, occurs when a country's currency experience\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class CFG:\n",
    "        seeds = [42, 119, 2020, 2024, 2028]\n",
    "        \n",
    "train_df, test_df, y, class_names = load_and_prepare_data()\n",
    "pairs_train, pairs_val, test_pairs, y_train, y_val = prepare_dual_conversation_pipeline(train_df, test_df, y)\n",
    "\n",
    "print(\"Conv A Example:\\n\", pairs_train[0][0][:200])\n",
    "print(\"Conv B Example:\\n\", pairs_train[0][1][:200])\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2742a2d4-9703-4121-b587-90d0c16518f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 01:51:44.860225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38367 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2025-11-16 01:51:44.862211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38367 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer (shared across the two inputs)\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 20000\n",
    "max_length = 512\n",
    "\n",
    "adapt_strings = [p[0] for p in pairs_train] + [p[1] for p in pairs_train]\n",
    "adapt_ds = tf.data.Dataset.from_tensor_slices([str(t) for t in adapt_strings]).batch(1024)\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorizer.adapt(adapt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473d51b2-47dd-41c2-8710-ae061016ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data pipelines for ((A,B), y)\n",
    "def make_dual_dataset(pairs, labels=None, batch_size=128, training=True):\n",
    "    part_a = [str(p[0]) for p in pairs]\n",
    "    part_b = [str(p[1]) for p in pairs]\n",
    "\n",
    "    inputs = {\"inp_a\": tf.constant(part_a), \"inp_b\": tf.constant(part_b)}\n",
    "\n",
    "    if labels is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    else:\n",
    "        labels = np.asarray(labels, dtype=np.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "    if labels is not None and training:\n",
    "        ds = ds.shuffle(2048, reshuffle_each_iteration=True)\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dual_dataset(pairs_train, y_train, training=True)\n",
    "val_ds   = make_dual_dataset(pairs_val,   y_val,   training=False)\n",
    "test_ds  = make_dual_dataset(test_pairs,  labels=None,  training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62a1b71-9c02-4b24-8b25-c79cec2bdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (two string inputs â†’ vectorizer â†’ shared embedding â†’ concat â†’ conv)\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D, Concatenate, Add\n",
    "\n",
    "def res_block(x, filters, use_projection=False):\n",
    "    shortcut = x\n",
    "    out = Conv1D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    out = Conv1D(filters, 3, padding=\"same\")(out)\n",
    "\n",
    "    if use_projection or shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding=\"same\")(shortcut)\n",
    "\n",
    "    out = Add()([out, shortcut])\n",
    "    out = tf.nn.relu(out)\n",
    "    return out\n",
    "    \n",
    "def conv_block(x, filters, use_residual=False, pool=True):\n",
    "    if use_residual:\n",
    "        x = res_block(x, filters, use_projection=True)\n",
    "    else:\n",
    "        x = Conv1D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    if pool:\n",
    "        x = MaxPooling1D()(x)\n",
    "    return x\n",
    "    \n",
    "def get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3):\n",
    "    inp_a = Input(shape=(), dtype=tf.string, name=\"inp_a\")\n",
    "    inp_b = Input(shape=(), dtype=tf.string, name=\"inp_b\")\n",
    "\n",
    "    # shared layers\n",
    "    emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    # branch A\n",
    "    xa = text_vectorizer(inp_a)\n",
    "    xa = emb(xa)\n",
    "    \n",
    "    # Conv32Ã—2 â†’ MP\n",
    "    xa = conv_block(xa, 32, use_residual=True, pool=True)\n",
    "\n",
    "    # Conv64Ã—2 â†’ MP\n",
    "    xa = conv_block(xa, 64, use_residual=True, pool=True)\n",
    "\n",
    "    # Conv128 â†’ GMP\n",
    "    xa = conv_block(xa, 128, use_residual=False, pool=False)\n",
    "    xa = GlobalMaxPooling1D()(xa)\n",
    "\n",
    "    # branch B\n",
    "    xb = text_vectorizer(inp_b)\n",
    "    xb = emb(xb)\n",
    "    \n",
    "    # Conv32Ã—2 â†’ MP\n",
    "    xb = conv_block(xb, 32, use_residual=True, pool=True)\n",
    "\n",
    "    # Conv64Ã—2 â†’ MP\n",
    "    xb = conv_block(xb, 64, use_residual=True, pool=True)\n",
    "    \n",
    "    # Conv128 â†’ GMP\n",
    "    xb = conv_block(xb, 128, use_residual=False, pool=False)\n",
    "    xb = GlobalMaxPooling1D()(xb)\n",
    "\n",
    "    # merge\n",
    "    x  = Concatenate()([xa, xb])\n",
    "    x  = Dropout(0.3)(x)\n",
    "    x  = Dense(256, activation=\"swish\")(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_a, inp_b], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da7f1d1-23be-41a0-9f7b-4c4c61bcc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training new model for seed 42...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 01:51:53.001183: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-16 01:51:53.463476: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-11-16 01:51:53.540136: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-16 01:51:54.717130: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ffcc546ad80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-16 01:51:54.717171: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2025-11-16 01:51:54.717178: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2025-11-16 01:51:54.723054: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763279514.795890 1090727 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - ETA: 0s - loss: 1.0909 - accuracy: 0.3832\n",
      "Epoch 1: val_loss improved from inf to 1.07809, saving model to models_cnn_dual2_skip/cnn_dual_seed_42.keras\n",
      "360/360 [==============================] - 44s 105ms/step - loss: 1.0909 - accuracy: 0.3832 - val_loss: 1.0781 - val_accuracy: 0.4243\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.4507\n",
      "Epoch 2: val_loss did not improve from 1.07809\n",
      "360/360 [==============================] - 26s 72ms/step - loss: 1.0547 - accuracy: 0.4507 - val_loss: 1.0790 - val_accuracy: 0.4293\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9439 - accuracy: 0.5444\n",
      "Epoch 3: val_loss did not improve from 1.07809\n",
      "360/360 [==============================] - 20s 55ms/step - loss: 0.9439 - accuracy: 0.5444 - val_loss: 1.1835 - val_accuracy: 0.4276\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.6697\n",
      "Epoch 4: val_loss did not improve from 1.07809\n",
      "360/360 [==============================] - 15s 43ms/step - loss: 0.7340 - accuracy: 0.6697 - val_loss: 1.3721 - val_accuracy: 0.4208\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.7780\n",
      "Epoch 5: val_loss did not improve from 1.07809\n",
      "360/360 [==============================] - 13s 37ms/step - loss: 0.5274 - accuracy: 0.7780 - val_loss: 1.8049 - val_accuracy: 0.4068\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8420\n",
      "Epoch 6: val_loss did not improve from 1.07809\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 12s 34ms/step - loss: 0.3951 - accuracy: 0.8420 - val_loss: 2.2700 - val_accuracy: 0.3997\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 42): 130.41 seconds (2.17 minutes)\n",
      "[Seed 42] Val Loss: 1.0781 | Val Acc: 42.43%\n",
      "\n",
      "ðŸš€ Training new model for seed 119...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0872 - accuracy: 0.3896\n",
      "Epoch 1: val_loss improved from inf to 1.08532, saving model to models_cnn_dual2_skip/cnn_dual_seed_119.keras\n",
      "360/360 [==============================] - 33s 83ms/step - loss: 1.0872 - accuracy: 0.3896 - val_loss: 1.0853 - val_accuracy: 0.4006\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0397 - accuracy: 0.4669\n",
      "Epoch 2: val_loss improved from 1.08532 to 1.08444, saving model to models_cnn_dual2_skip/cnn_dual_seed_119.keras\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 1.0397 - accuracy: 0.4669 - val_loss: 1.0844 - val_accuracy: 0.4326\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8855 - accuracy: 0.5799\n",
      "Epoch 3: val_loss did not improve from 1.08444\n",
      "360/360 [==============================] - 18s 49ms/step - loss: 0.8855 - accuracy: 0.5799 - val_loss: 1.2552 - val_accuracy: 0.4099\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.7106\n",
      "Epoch 4: val_loss did not improve from 1.08444\n",
      "360/360 [==============================] - 15s 40ms/step - loss: 0.6556 - accuracy: 0.7106 - val_loss: 1.7333 - val_accuracy: 0.4010\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.8010\n",
      "Epoch 5: val_loss did not improve from 1.08444\n",
      "360/360 [==============================] - 12s 32ms/step - loss: 0.4787 - accuracy: 0.8010 - val_loss: 1.9933 - val_accuracy: 0.4116\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.8592\n",
      "Epoch 6: val_loss did not improve from 1.08444\n",
      "360/360 [==============================] - 10s 29ms/step - loss: 0.3552 - accuracy: 0.8592 - val_loss: 2.5062 - val_accuracy: 0.4140\n",
      "Epoch 7/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.8948\n",
      "Epoch 7: val_loss did not improve from 1.08444\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "360/360 [==============================] - 10s 28ms/step - loss: 0.2743 - accuracy: 0.8948 - val_loss: 2.8822 - val_accuracy: 0.4015\n",
      "Epoch 7: early stopping\n",
      "â±ï¸ CNN Training time (seed 119): 120.53 seconds (2.01 minutes)\n",
      "[Seed 119] Val Loss: 1.0844 | Val Acc: 43.26%\n",
      "\n",
      "ðŸš€ Training new model for seed 2020...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0924 - accuracy: 0.3757\n",
      "Epoch 1: val_loss improved from inf to 1.08346, saving model to models_cnn_dual2_skip/cnn_dual_seed_2020.keras\n",
      "360/360 [==============================] - 32s 80ms/step - loss: 1.0924 - accuracy: 0.3757 - val_loss: 1.0835 - val_accuracy: 0.4072\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0604 - accuracy: 0.4448\n",
      "Epoch 2: val_loss improved from 1.08346 to 1.08118, saving model to models_cnn_dual2_skip/cnn_dual_seed_2020.keras\n",
      "360/360 [==============================] - 22s 61ms/step - loss: 1.0604 - accuracy: 0.4448 - val_loss: 1.0812 - val_accuracy: 0.4212\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9667 - accuracy: 0.5282\n",
      "Epoch 3: val_loss did not improve from 1.08118\n",
      "360/360 [==============================] - 17s 46ms/step - loss: 0.9667 - accuracy: 0.5282 - val_loss: 1.1727 - val_accuracy: 0.4046\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7793 - accuracy: 0.6440\n",
      "Epoch 4: val_loss did not improve from 1.08118\n",
      "360/360 [==============================] - 15s 41ms/step - loss: 0.7793 - accuracy: 0.6440 - val_loss: 1.5033 - val_accuracy: 0.4023\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5847 - accuracy: 0.7496\n",
      "Epoch 5: val_loss did not improve from 1.08118\n",
      "360/360 [==============================] - 14s 37ms/step - loss: 0.5847 - accuracy: 0.7496 - val_loss: 1.8986 - val_accuracy: 0.3883\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8161\n",
      "Epoch 6: val_loss did not improve from 1.08118\n",
      "360/360 [==============================] - 10s 27ms/step - loss: 0.4536 - accuracy: 0.8161 - val_loss: 2.1512 - val_accuracy: 0.3907\n",
      "Epoch 7/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8645\n",
      "Epoch 7: val_loss did not improve from 1.08118\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "360/360 [==============================] - 9s 25ms/step - loss: 0.3478 - accuracy: 0.8645 - val_loss: 2.4022 - val_accuracy: 0.3837\n",
      "Epoch 7: early stopping\n",
      "â±ï¸ CNN Training time (seed 2020): 117.81 seconds (1.96 minutes)\n",
      "[Seed 2020] Val Loss: 1.0812 | Val Acc: 42.12%\n",
      "\n",
      "ðŸš€ Training new model for seed 2024...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0906 - accuracy: 0.3809\n",
      "Epoch 1: val_loss improved from inf to 1.09300, saving model to models_cnn_dual2_skip/cnn_dual_seed_2024.keras\n",
      "360/360 [==============================] - 33s 82ms/step - loss: 1.0906 - accuracy: 0.3809 - val_loss: 1.0930 - val_accuracy: 0.3792\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0563 - accuracy: 0.4469\n",
      "Epoch 2: val_loss improved from 1.09300 to 1.07481, saving model to models_cnn_dual2_skip/cnn_dual_seed_2024.keras\n",
      "360/360 [==============================] - 22s 62ms/step - loss: 1.0563 - accuracy: 0.4469 - val_loss: 1.0748 - val_accuracy: 0.4294\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9524 - accuracy: 0.5359\n",
      "Epoch 3: val_loss did not improve from 1.07481\n",
      "360/360 [==============================] - 16s 44ms/step - loss: 0.9524 - accuracy: 0.5359 - val_loss: 1.1763 - val_accuracy: 0.4214\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7606 - accuracy: 0.6539\n",
      "Epoch 4: val_loss did not improve from 1.07481\n",
      "360/360 [==============================] - 13s 37ms/step - loss: 0.7606 - accuracy: 0.6539 - val_loss: 1.5018 - val_accuracy: 0.4055\n",
      "Epoch 5/30\n",
      "358/360 [============================>.] - ETA: 0s - loss: 0.5701 - accuracy: 0.7557\n",
      "Epoch 5: val_loss did not improve from 1.07481\n",
      "360/360 [==============================] - 11s 32ms/step - loss: 0.5696 - accuracy: 0.7557 - val_loss: 1.7827 - val_accuracy: 0.3909\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8312\n",
      "Epoch 6: val_loss did not improve from 1.07481\n",
      "360/360 [==============================] - 9s 26ms/step - loss: 0.4196 - accuracy: 0.8312 - val_loss: 2.3149 - val_accuracy: 0.3790\n",
      "Epoch 7/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8761\n",
      "Epoch 7: val_loss did not improve from 1.07481\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "360/360 [==============================] - 9s 25ms/step - loss: 0.3223 - accuracy: 0.8761 - val_loss: 2.5787 - val_accuracy: 0.3782\n",
      "Epoch 7: early stopping\n",
      "â±ï¸ CNN Training time (seed 2024): 114.22 seconds (1.90 minutes)\n",
      "[Seed 2024] Val Loss: 1.0748 | Val Acc: 42.94%\n",
      "\n",
      "ðŸš€ Training new model for seed 2028...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0895 - accuracy: 0.3826\n",
      "Epoch 1: val_loss improved from inf to 1.07716, saving model to models_cnn_dual2_skip/cnn_dual_seed_2028.keras\n",
      "360/360 [==============================] - 33s 83ms/step - loss: 1.0895 - accuracy: 0.3826 - val_loss: 1.0772 - val_accuracy: 0.4073\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0446 - accuracy: 0.4608\n",
      "Epoch 2: val_loss improved from 1.07716 to 1.07113, saving model to models_cnn_dual2_skip/cnn_dual_seed_2028.keras\n",
      "360/360 [==============================] - 21s 58ms/step - loss: 1.0446 - accuracy: 0.4608 - val_loss: 1.0711 - val_accuracy: 0.4455\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9119 - accuracy: 0.5658\n",
      "Epoch 3: val_loss did not improve from 1.07113\n",
      "360/360 [==============================] - 17s 48ms/step - loss: 0.9119 - accuracy: 0.5658 - val_loss: 1.2031 - val_accuracy: 0.4354\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.6945\n",
      "Epoch 4: val_loss did not improve from 1.07113\n",
      "360/360 [==============================] - 15s 41ms/step - loss: 0.6913 - accuracy: 0.6945 - val_loss: 1.5804 - val_accuracy: 0.4203\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7881\n",
      "Epoch 5: val_loss did not improve from 1.07113\n",
      "360/360 [==============================] - 11s 32ms/step - loss: 0.5050 - accuracy: 0.7881 - val_loss: 2.0226 - val_accuracy: 0.4073\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.3805 - accuracy: 0.8456\n",
      "Epoch 6: val_loss did not improve from 1.07113\n",
      "360/360 [==============================] - 11s 29ms/step - loss: 0.3805 - accuracy: 0.8456 - val_loss: 2.4736 - val_accuracy: 0.3982\n",
      "Epoch 7/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.8892\n",
      "Epoch 7: val_loss did not improve from 1.07113\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "360/360 [==============================] - 10s 27ms/step - loss: 0.2854 - accuracy: 0.8892 - val_loss: 2.8867 - val_accuracy: 0.3981\n",
      "Epoch 7: early stopping\n",
      "â±ï¸ CNN Training time (seed 2028): 117.98 seconds (1.97 minutes)\n",
      "[Seed 2028] Val Loss: 1.0711 | Val Acc: 44.55%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train one model or an ensemble over seeds\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "os.makedirs(\"models_cnn_dual2_skip_drop\", exist_ok=True)\n",
    "cnn_models = []\n",
    "\n",
    "for seed in CFG.seeds:\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    model_path = os.path.join(\"models_cnn_dual2_skip_drop\", f\"cnn_dual_seed_{seed}.keras\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"âœ… Found existing model for seed {seed}, loading...\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        print(f\"ðŸš€ Training new model for seed {seed}...\")\n",
    "        model = get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Measure training time ---\n",
    "        t0 = time.time()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=30,\n",
    "            callbacks=[ckpt, es],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        t1 = time.time()\n",
    "        print(f\"â±ï¸ CNN Training time (seed {seed}): {(t1 - t0):.2f} seconds ({(t1 - t0)/60:.2f} minutes)\")\n",
    "        \n",
    "        # --- Reload best model (for consistency) ---\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # --- Evaluate on validation set ---\n",
    "    loss, acc = model.evaluate(val_ds, verbose=0)\n",
    "    print(f\"[Seed {seed}] Val Loss: {loss:.4f} | Val Acc: {acc*100:.2f}%\\n\")\n",
    "\n",
    "    # --- Store model for ensemble ---\n",
    "    cnn_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56514fb-870a-4c61-b9b3-c764b67ebd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in cnn_models: 5\n",
      "Model 0 predictions shape: (11496, 3)\n",
      "Model 1 predictions shape: (11496, 3)\n",
      "Model 2 predictions shape: (11496, 3)\n",
      "Model 3 predictions shape: (11496, 3)\n",
      "Model 4 predictions shape: (11496, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of models in cnn_models: {len(cnn_models)}\")\n",
    "if len(cnn_models) > 0:\n",
    "    for i, m in enumerate(cnn_models):\n",
    "        preds = m.predict(val_ds, verbose=0)\n",
    "        print(f\"Model {i} predictions shape:\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc2bc23-f8e5-4abb-b723-d8464a6806cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (CNN)\n",
    "y_proba_val_cnn = np.mean([m.predict(val_ds,  verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_val_cnn  = y_proba_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Test predictions + submission\n",
    "test_ds = make_dual_dataset(test_pairs, labels=None, training=False)\n",
    "\n",
    "y_proba_test_cnn = np.mean([m.predict(test_ds, verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_test_cnn  = y_proba_test_cnn.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9f0b37-b10a-4f5b-bfa2-6906fcdd2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ CNN2-Skip EVALUATION ================\n",
      "\n",
      "*** GLOBAL METRICS ***\n",
      "Accuracy (Global)      : 0.4538\n",
      "Precision (Macro Avg)  : 0.4557\n",
      "Recall (Macro Avg)     : 0.4450\n",
      "F1-Score (Macro Avg)   : 0.4309\n",
      "\n",
      "*** PER-CLASS EVALUATION ***\n",
      "Class                Precision    Recall  F1-Score   Support\n",
      "------------------------------------------------------------\n",
      "winner_model_a            0.44      0.64      0.52      4013\n",
      "winner_model_b            0.47      0.48      0.48      3931\n",
      "winner_tie                0.45      0.22      0.30      3552\n",
      "------------------------------------------------------------\n",
      "Macro Avg                 0.46      0.45      0.43     34488\n",
      "Weighted Avg              0.46      0.45      0.44     34488\n",
      "\n",
      "*** ROC-AUC EVALUATION ***\n",
      "ROC-AUC (OvR) : 0.6251\n",
      "\n",
      "*** LOG-LOSS EVALUATION ***\n",
      "Log-loss      : 1.0537\n",
      "\n",
      "*** LOG-LOSS PER CLASS ***\n",
      "Class 0: 0.9592  (n=4013)\n",
      "Class 1: 1.0484  (n=3931)\n",
      "Class 2: 1.1663  (n=3552)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ CNN2-Skip EVALUATION ================\\n\")\n",
    "# Metrics\n",
    "_ = eval_metrics(y_val, y_pred_val_cnn)\n",
    "eval_classification_report(y_val, y_pred_val_cnn, class_names)\n",
    "# ROC-AUC\n",
    "_ = eval_roc_auc(y_val, y_proba_val_cnn)\n",
    "# Log-loss\n",
    "_ = eval_log_loss(y_val, y_proba_val_cnn)\n",
    "_ = eval_log_loss_per_class(y_val, y_proba_val_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880a1e6c-1a5b-436f-bed9-2f29d04433f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      " [[2550 1004  459]\n",
      " [1565 1888  478]\n",
      " [1666 1107  779]]\n",
      "Saved plot to: images/confusion_matrix/confusion_matrix_cnn2s.png\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix + Plot\n",
    "cm_cnn = eval_confusion_matrix(y_val, y_pred_val_cnn, n_classes=y_proba_val_cnn.shape[1])\n",
    "plot_confusion_matrix(cm_cnn, class_names, title=\"Confusion Matrix â€” CNN2S\", save_path=\"results/confusion_matrix/confusion_matrix_cnn2s.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4f7042-724d-48b0-9a67-9797aac5ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: images/roc/roc_cnn2s.png\n"
     ]
    }
   ],
   "source": [
    "# ROC Curves\n",
    "plot_roc_curves(y_val, y_proba_val_cnn, class_names, title_prefix=\"CNN2S ROC\", save_path=\"results/roc/roc_cnn2s.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91e46aa-3068-40d2-89e9-46b1406caade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC data for class 0 (AUC=0.6429) â†’ results/roc/CNN2S_fold1_class0.csv\n",
      "Saved ROC data for class 1 (AUC=0.6411) â†’ results/roc/CNN2S_fold1_class1.csv\n",
      "Saved ROC data for class 2 (AUC=0.5912) â†’ results/roc/CNN2S_fold1_class2.csv\n"
     ]
    }
   ],
   "source": [
    "save_roc_to_csv(y_val, y_proba_val_cnn, \"CNN2S\", fold_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb6dc0b-095c-47af-9c1e-4328cc34bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: results/submission/submission_cnn2s.csv\n"
     ]
    }
   ],
   "source": [
    "submission_cnn = build_submission(\n",
    "    test_df=test_df,\n",
    "    y_pred_test=y_pred_test_cnn,\n",
    "    y_proba_test=y_proba_test_cnn,\n",
    "    model_name=\"cnn2s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649f1bb-64aa-471b-87f8-253c4d98c253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
