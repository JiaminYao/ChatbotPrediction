{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a04504e-4021-45ae-8788-580d052d089c",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c3f52-e1fb-4978-81ce-933f9c928358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 23:17:47.522599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-02 23:17:47.522669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-02 23:17:47.524434: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-02 23:17:47.533862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import os, tensorflow as tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "from evaluation import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92942f37-3dbc-4623-aed3-d86bbd2debad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    TextVectorization, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "    Dense, Dropout, Input, MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d36760-c4a4-4fe8-95d5-169bdd75e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv A Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis refers to a situation where a country faces severe shortage of foreign currencies, usually dollars or euros\n",
      "Conv B Example:\n",
      " What is a foreign exchange crisis? What are some notable examples?\n",
      "A foreign exchange crisis, also known as a currency crisis or balance of payments crisis, occurs when a country's currency experience\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    class CFG:\n",
    "        seeds = [42, 119, 2020, 2024, 2028]\n",
    "        \n",
    "train_df, test_df, y, class_names = load_and_prepare_data()\n",
    "pairs_train, pairs_val, test_pairs, y_train, y_val = prepare_dual_conversation_pipeline(train_df, test_df, y)\n",
    "\n",
    "print(\"Conv A Example:\\n\", pairs_train[0][0][:200])\n",
    "print(\"Conv B Example:\\n\", pairs_train[0][1][:200])\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2742a2d4-9703-4121-b587-90d0c16518f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 23:18:03.283836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 626 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer (shared across the two inputs)\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 20000\n",
    "max_length = 512\n",
    "\n",
    "adapt_strings = [p[0] for p in pairs_train] + [p[1] for p in pairs_train]\n",
    "adapt_ds = tf.data.Dataset.from_tensor_slices([str(t) for t in adapt_strings]).batch(1024)\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorizer.adapt(adapt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473d51b2-47dd-41c2-8710-ae061016ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data pipelines for ((A,B), y)\n",
    "def make_dual_dataset(pairs, labels=None, batch_size=128, training=True):\n",
    "    part_a = [str(p[0]) for p in pairs]\n",
    "    part_b = [str(p[1]) for p in pairs]\n",
    "\n",
    "    inputs = {\"inp_a\": tf.constant(part_a), \"inp_b\": tf.constant(part_b)}\n",
    "\n",
    "    if labels is None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    else:\n",
    "        labels = np.asarray(labels, dtype=np.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "    if labels is not None and training:\n",
    "        ds = ds.shuffle(2048, reshuffle_each_iteration=True)\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dual_dataset(pairs_train, y_train, training=True)\n",
    "val_ds   = make_dual_dataset(pairs_val,   y_val,   training=False)\n",
    "test_ds  = make_dual_dataset(test_pairs,  labels=None,  training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62a1b71-9c02-4b24-8b25-c79cec2bdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (two string inputs â†’ vectorizer â†’ shared embedding â†’ concat â†’ conv)\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3):\n",
    "    inp_a = Input(shape=(), dtype=tf.string, name=\"inp_a\")\n",
    "    inp_b = Input(shape=(), dtype=tf.string, name=\"inp_b\")\n",
    "\n",
    "    # shared layers\n",
    "    emb   = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    # branch A\n",
    "    xa = text_vectorizer(inp_a)\n",
    "    xa = emb(xa)\n",
    "    xa = Conv1D(32, 3, activation=\"relu\")(xa)\n",
    "    xa = Conv1D(32, 3, activation=\"relu\")(xa)\n",
    "    xa = MaxPooling1D()(xa)\n",
    "    xa = Conv1D(64, 3, activation=\"relu\")(xa)\n",
    "    xa = Conv1D(64, 3, activation=\"relu\")(xa)\n",
    "    xa = MaxPooling1D()(xa)\n",
    "    xa = Conv1D(128, 3, activation=\"relu\")(xa)\n",
    "    xa = Conv1D(128, 3, activation=\"relu\")(xa)\n",
    "    xa = GlobalMaxPooling1D()(xa)\n",
    "\n",
    "    # branch B\n",
    "    xb = text_vectorizer(inp_b)\n",
    "    xb = emb(xb)\n",
    "    xb = Conv1D(32, 3, activation=\"relu\")(xb)\n",
    "    xb = Conv1D(32, 3, activation=\"relu\")(xb)\n",
    "    xb = MaxPooling1D()(xb)\n",
    "    xb = Conv1D(64, 3, activation=\"relu\")(xb)\n",
    "    xb = Conv1D(64, 3, activation=\"relu\")(xb)\n",
    "    xb = MaxPooling1D()(xb)\n",
    "    xb = Conv1D(128, 3, activation=\"relu\")(xb)\n",
    "    xb = Conv1D(128, 3, activation=\"relu\")(xb)\n",
    "    xb = GlobalMaxPooling1D()(xb)\n",
    "\n",
    "    # merge\n",
    "    x  = Concatenate()([xa, xb])\n",
    "    x  = Dropout(0.3)(x)\n",
    "    x  = Dense(256, activation=\"swish\")(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp_a, inp_b], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da7f1d1-23be-41a0-9f7b-4c4c61bcc7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training new model for seed 42...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 23:18:11.053422: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-02 23:18:11.524998: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-11-02 23:18:11.612228: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-11-02 23:18:12.765254: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ffcf42e8210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-02 23:18:12.765291: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2025-11-02 23:18:12.772240: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762147092.853892 2799103 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-11-02 23:18:13.254023: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-02 23:18:13.263332: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-02 23:18:13.575304: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-02 23:18:13.582465: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-02 23:18:14.194549: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-02 23:18:14.206479: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - ETA: 0s - loss: 1.0937 - accuracy: 0.3745\n",
      "Epoch 1: val_loss improved from inf to 1.08479, saving model to models_cnn_dual3/cnn_dual_seed_42.keras\n",
      "360/360 [==============================] - 44s 107ms/step - loss: 1.0937 - accuracy: 0.3745 - val_loss: 1.0848 - val_accuracy: 0.4035\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0569 - accuracy: 0.4464\n",
      "Epoch 2: val_loss did not improve from 1.08479\n",
      "360/360 [==============================] - 27s 74ms/step - loss: 1.0569 - accuracy: 0.4464 - val_loss: 1.1020 - val_accuracy: 0.4122\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.5522\n",
      "Epoch 3: val_loss did not improve from 1.08479\n",
      "360/360 [==============================] - 22s 61ms/step - loss: 0.9335 - accuracy: 0.5522 - val_loss: 1.2343 - val_accuracy: 0.4006\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7716 - accuracy: 0.6449\n",
      "Epoch 4: val_loss did not improve from 1.08479\n",
      "360/360 [==============================] - 18s 49ms/step - loss: 0.7716 - accuracy: 0.6449 - val_loss: 1.6264 - val_accuracy: 0.3994\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.7213\n",
      "Epoch 5: val_loss did not improve from 1.08479\n",
      "360/360 [==============================] - 14s 38ms/step - loss: 0.6340 - accuracy: 0.7213 - val_loss: 1.8058 - val_accuracy: 0.3827\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.7655\n",
      "Epoch 6: val_loss did not improve from 1.08479\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 12s 33ms/step - loss: 0.5481 - accuracy: 0.7655 - val_loss: 2.0085 - val_accuracy: 0.3908\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 42): 136.49 seconds (2.27 minutes)\n",
      "[Seed 42] Val Loss: 1.0848 | Val Acc: 40.35%\n",
      "\n",
      "ðŸš€ Training new model for seed 119...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0976 - accuracy: 0.3481\n",
      "Epoch 1: val_loss improved from inf to 1.09638, saving model to models_cnn_dual3/cnn_dual_seed_119.keras\n",
      "360/360 [==============================] - 34s 86ms/step - loss: 1.0976 - accuracy: 0.3481 - val_loss: 1.0964 - val_accuracy: 0.3419\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0908 - accuracy: 0.3731\n",
      "Epoch 2: val_loss did not improve from 1.09638\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 1.0908 - accuracy: 0.3731 - val_loss: 1.0972 - val_accuracy: 0.3779\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0517 - accuracy: 0.4364\n",
      "Epoch 3: val_loss did not improve from 1.09638\n",
      "360/360 [==============================] - 17s 47ms/step - loss: 1.0517 - accuracy: 0.4364 - val_loss: 1.1025 - val_accuracy: 0.3634\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9591 - accuracy: 0.5165\n",
      "Epoch 4: val_loss did not improve from 1.09638\n",
      "360/360 [==============================] - 15s 43ms/step - loss: 0.9591 - accuracy: 0.5165 - val_loss: 1.2356 - val_accuracy: 0.3776\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8408 - accuracy: 0.5936\n",
      "Epoch 5: val_loss did not improve from 1.09638\n",
      "360/360 [==============================] - 13s 35ms/step - loss: 0.8408 - accuracy: 0.5936 - val_loss: 1.3449 - val_accuracy: 0.3880\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.6611\n",
      "Epoch 6: val_loss did not improve from 1.09638\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 11s 31ms/step - loss: 0.7281 - accuracy: 0.6611 - val_loss: 1.6327 - val_accuracy: 0.3919\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 119): 113.44 seconds (1.89 minutes)\n",
      "[Seed 119] Val Loss: 1.0964 | Val Acc: 34.19%\n",
      "\n",
      "ðŸš€ Training new model for seed 2020...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0967 - accuracy: 0.3499\n",
      "Epoch 1: val_loss improved from inf to 1.09336, saving model to models_cnn_dual3/cnn_dual_seed_2020.keras\n",
      "360/360 [==============================] - 34s 85ms/step - loss: 1.0967 - accuracy: 0.3499 - val_loss: 1.0934 - val_accuracy: 0.3673\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0833 - accuracy: 0.3918\n",
      "Epoch 2: val_loss did not improve from 1.09336\n",
      "360/360 [==============================] - 22s 61ms/step - loss: 1.0833 - accuracy: 0.3918 - val_loss: 1.0991 - val_accuracy: 0.3717\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0417 - accuracy: 0.4458\n",
      "Epoch 3: val_loss did not improve from 1.09336\n",
      "360/360 [==============================] - 18s 50ms/step - loss: 1.0417 - accuracy: 0.4458 - val_loss: 1.1491 - val_accuracy: 0.3655\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9737 - accuracy: 0.5099\n",
      "Epoch 4: val_loss did not improve from 1.09336\n",
      "360/360 [==============================] - 15s 41ms/step - loss: 0.9737 - accuracy: 0.5099 - val_loss: 1.1979 - val_accuracy: 0.3479\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8909 - accuracy: 0.5699\n",
      "Epoch 5: val_loss did not improve from 1.09336\n",
      "360/360 [==============================] - 11s 32ms/step - loss: 0.8909 - accuracy: 0.5699 - val_loss: 1.2967 - val_accuracy: 0.3496\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8046 - accuracy: 0.6241\n",
      "Epoch 6: val_loss did not improve from 1.09336\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 10s 28ms/step - loss: 0.8046 - accuracy: 0.6241 - val_loss: 1.4710 - val_accuracy: 0.3622\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 2020): 109.84 seconds (1.83 minutes)\n",
      "[Seed 2020] Val Loss: 1.0934 | Val Acc: 36.73%\n",
      "\n",
      "ðŸš€ Training new model for seed 2024...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0964 - accuracy: 0.3572\n",
      "Epoch 1: val_loss improved from inf to 1.09061, saving model to models_cnn_dual3/cnn_dual_seed_2024.keras\n",
      "360/360 [==============================] - 34s 86ms/step - loss: 1.0964 - accuracy: 0.3572 - val_loss: 1.0906 - val_accuracy: 0.3764\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.4129\n",
      "Epoch 2: val_loss improved from 1.09061 to 1.08722, saving model to models_cnn_dual3/cnn_dual_seed_2024.keras\n",
      "360/360 [==============================] - 23s 64ms/step - loss: 1.0778 - accuracy: 0.4129 - val_loss: 1.0872 - val_accuracy: 0.4088\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9975 - accuracy: 0.5050\n",
      "Epoch 3: val_loss did not improve from 1.08722\n",
      "360/360 [==============================] - 17s 48ms/step - loss: 0.9975 - accuracy: 0.5050 - val_loss: 1.2015 - val_accuracy: 0.4050\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.8620 - accuracy: 0.6008\n",
      "Epoch 4: val_loss did not improve from 1.08722\n",
      "360/360 [==============================] - 15s 41ms/step - loss: 0.8620 - accuracy: 0.6008 - val_loss: 1.3471 - val_accuracy: 0.3905\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7287 - accuracy: 0.6770\n",
      "Epoch 5: val_loss did not improve from 1.08722\n",
      "360/360 [==============================] - 11s 31ms/step - loss: 0.7287 - accuracy: 0.6770 - val_loss: 1.5571 - val_accuracy: 0.3915\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.7258\n",
      "Epoch 6: val_loss did not improve from 1.08722\n",
      "360/360 [==============================] - 10s 29ms/step - loss: 0.6354 - accuracy: 0.7258 - val_loss: 1.7272 - val_accuracy: 0.3774\n",
      "Epoch 7/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.7585\n",
      "Epoch 7: val_loss did not improve from 1.08722\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "360/360 [==============================] - 11s 30ms/step - loss: 0.5682 - accuracy: 0.7585 - val_loss: 1.9676 - val_accuracy: 0.3823\n",
      "Epoch 7: early stopping\n",
      "â±ï¸ CNN Training time (seed 2024): 121.12 seconds (2.02 minutes)\n",
      "[Seed 2024] Val Loss: 1.0872 | Val Acc: 40.88%\n",
      "\n",
      "ðŸš€ Training new model for seed 2028...\n",
      "Epoch 1/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0929 - accuracy: 0.3739\n",
      "Epoch 1: val_loss improved from inf to 1.08831, saving model to models_cnn_dual3/cnn_dual_seed_2028.keras\n",
      "360/360 [==============================] - 34s 85ms/step - loss: 1.0929 - accuracy: 0.3739 - val_loss: 1.0883 - val_accuracy: 0.3925\n",
      "Epoch 2/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 1.0456 - accuracy: 0.4609\n",
      "Epoch 2: val_loss did not improve from 1.08831\n",
      "360/360 [==============================] - 22s 62ms/step - loss: 1.0456 - accuracy: 0.4609 - val_loss: 1.0981 - val_accuracy: 0.4198\n",
      "Epoch 3/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.5699\n",
      "Epoch 3: val_loss did not improve from 1.08831\n",
      "360/360 [==============================] - 17s 46ms/step - loss: 0.9047 - accuracy: 0.5699 - val_loss: 1.2438 - val_accuracy: 0.4036\n",
      "Epoch 4/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.6758\n",
      "Epoch 4: val_loss did not improve from 1.08831\n",
      "360/360 [==============================] - 14s 39ms/step - loss: 0.7190 - accuracy: 0.6758 - val_loss: 1.5146 - val_accuracy: 0.3868\n",
      "Epoch 5/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.7403\n",
      "Epoch 5: val_loss did not improve from 1.08831\n",
      "360/360 [==============================] - 12s 34ms/step - loss: 0.5956 - accuracy: 0.7403 - val_loss: 1.7843 - val_accuracy: 0.3892\n",
      "Epoch 6/30\n",
      "360/360 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.7772\n",
      "Epoch 6: val_loss did not improve from 1.08831\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "360/360 [==============================] - 10s 27ms/step - loss: 0.5224 - accuracy: 0.7772 - val_loss: 2.2689 - val_accuracy: 0.3921\n",
      "Epoch 6: early stopping\n",
      "â±ï¸ CNN Training time (seed 2028): 108.43 seconds (1.81 minutes)\n",
      "[Seed 2028] Val Loss: 1.0883 | Val Acc: 39.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train one model or an ensemble over seeds\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "os.makedirs(\"models_cnn_dual3\", exist_ok=True)\n",
    "cnn_models = []\n",
    "\n",
    "for seed in CFG.seeds:\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    model_path = os.path.join(\"models_cnn_dual3\", f\"cnn_dual_seed_{seed}.keras\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"âœ… Found existing model for seed {seed}, loading...\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        print(f\"ðŸš€ Training new model for seed {seed}...\")\n",
    "        model = get_dual_cnn_model(vocab_size=vocab_size, embed_dim=64, num_classes=3)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=model_path,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Measure training time ---\n",
    "        t0 = time.time()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=30,\n",
    "            callbacks=[ckpt, es],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        t1 = time.time()\n",
    "        print(f\"â±ï¸ CNN Training time (seed {seed}): {(t1 - t0):.2f} seconds ({(t1 - t0)/60:.2f} minutes)\")\n",
    "        \n",
    "        # --- Reload best model (for consistency) ---\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # --- Evaluate on validation set ---\n",
    "    loss, acc = model.evaluate(val_ds, verbose=0)\n",
    "    print(f\"[Seed {seed}] Val Loss: {loss:.4f} | Val Acc: {acc*100:.2f}%\\n\")\n",
    "\n",
    "    # --- Store model for ensemble ---\n",
    "    cnn_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56514fb-870a-4c61-b9b3-c764b67ebd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in cnn_models: 5\n",
      "Model 0 predictions shape: (11496, 3)\n",
      "Model 1 predictions shape: (11496, 3)\n",
      "Model 2 predictions shape: (11496, 3)\n",
      "Model 3 predictions shape: (11496, 3)\n",
      "Model 4 predictions shape: (11496, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of models in cnn_models: {len(cnn_models)}\")\n",
    "if len(cnn_models) > 0:\n",
    "    for i, m in enumerate(cnn_models):\n",
    "        preds = m.predict(val_ds, verbose=0)\n",
    "        print(f\"Model {i} predictions shape:\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc2bc23-f8e5-4abb-b723-d8464a6806cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (CNN)\n",
    "y_proba_val_cnn = np.mean([m.predict(val_ds,  verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_val_cnn  = y_proba_val_cnn.argmax(axis=1)\n",
    "\n",
    "# Test predictions + submission\n",
    "test_ds = make_dual_dataset(test_pairs, labels=None, training=False)\n",
    "\n",
    "y_proba_test_cnn = np.mean([m.predict(test_ds, verbose=0) for m in cnn_models], axis=0)\n",
    "y_pred_test_cnn  = y_proba_test_cnn.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9f0b37-b10a-4f5b-bfa2-6906fcdd2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ CNN3 EVALUATION ================\n",
      "\n",
      "*** GLOBAL METRICS ***\n",
      "Accuracy (Global)      : 0.4228\n",
      "Precision (Macro Avg)  : 0.4317\n",
      "Recall (Macro Avg)     : 0.4139\n",
      "F1-Score (Macro Avg)   : 0.3931\n",
      "\n",
      "*** PER-CLASS EVALUATION ***\n",
      "Class                Precision    Recall  F1-Score   Support\n",
      "------------------------------------------------------------\n",
      "winner_model_a            0.43      0.50      0.46      4013\n",
      "winner_model_b            0.41      0.58      0.48      3931\n",
      "winner_tie                0.46      0.16      0.24      3552\n",
      "------------------------------------------------------------\n",
      "Macro Avg                 0.43      0.41      0.39     34488\n",
      "Weighted Avg              0.43      0.42      0.40     34488\n",
      "\n",
      "*** ROC-AUC EVALUATION ***\n",
      "ROC-AUC (OvR) : 0.5936\n",
      "\n",
      "*** LOG-LOSS EVALUATION ***\n",
      "Log-loss      : 1.0835\n",
      "\n",
      "*** LOG-LOSS PER CLASS ***\n",
      "Class 0: 1.0429  (n=4013)\n",
      "Class 1: 1.0379  (n=3931)\n",
      "Class 2: 1.1800  (n=3552)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================ CNN3 EVALUATION ================\\n\")\n",
    "# Metrics\n",
    "_ = eval_metrics(y_val, y_pred_val_cnn)\n",
    "eval_classification_report(y_val, y_pred_val_cnn, class_names)\n",
    "# ROC-AUC\n",
    "_ = eval_roc_auc(y_val, y_proba_val_cnn)\n",
    "# Log-loss\n",
    "_ = eval_log_loss(y_val, y_proba_val_cnn)\n",
    "_ = eval_log_loss_per_class(y_val, y_proba_val_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880a1e6c-1a5b-436f-bed9-2f29d04433f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      " [[2000 1664  349]\n",
      " [1311 2294  326]\n",
      " [1391 1594  567]]\n",
      "Saved plot to: images/confusion_matrix/confusion_matrix_cnn3.png\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix + Plot\n",
    "cm_cnn = eval_confusion_matrix(y_val, y_pred_val_cnn, n_classes=y_proba_val_cnn.shape[1])\n",
    "plot_confusion_matrix(cm_cnn, class_names, title=\"Confusion Matrix â€” CNN3\", save_path=\"results/confusion_matrix/confusion_matrix_cnn3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4f7042-724d-48b0-9a67-9797aac5ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: images/roc/roc_cnn3.png\n"
     ]
    }
   ],
   "source": [
    "# ROC Curves\n",
    "plot_roc_curves(y_val, y_proba_val_cnn, class_names, title_prefix=\"CNN3 ROC\", save_path=\"results/roc/roc_cnn3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91e46aa-3068-40d2-89e9-46b1406caade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC data for class 0 (AUC=0.6069) â†’ results/roc/CNN3_fold1_class0.csv\n",
      "Saved ROC data for class 1 (AUC=0.6085) â†’ results/roc/CNN3_fold1_class1.csv\n",
      "Saved ROC data for class 2 (AUC=0.5652) â†’ results/roc/CNN3_fold1_class2.csv\n"
     ]
    }
   ],
   "source": [
    "save_roc_to_csv(y_val, y_proba_val_cnn, \"CNN3\", fold_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb6dc0b-095c-47af-9c1e-4328cc34bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: results/submission/submission_cnn3.csv\n"
     ]
    }
   ],
   "source": [
    "submission_cnn = build_submission(\n",
    "    test_df=test_df,\n",
    "    y_pred_test=y_pred_test_cnn,\n",
    "    y_proba_test=y_proba_test_cnn,\n",
    "    model_name=\"cnn3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649f1bb-64aa-471b-87f8-253c4d98c253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
